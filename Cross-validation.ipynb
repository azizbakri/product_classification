{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gensim.models\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42) \n",
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "# Must specify y StratifiedKFold for\n",
    "for train, test in kf.split(X,df['Class']):  \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train1 = X[train]\n",
    "    y_train1 = dummy_y[train]\n",
    "    x_test1 = X[test]\n",
    "    y_test1 = dummy_y[test]\n",
    "    sentences1 = [row.split(' ') for row in x_train1]\n",
    "    model1 = gensim.models.Word2Vec(sentences1, min_count=1,size= 100,workers=3, window =3, sg = 1)\n",
    "\n",
    "    tokenizer1 = Tokenizer()\n",
    "    tokenizer1.fit_on_texts(x_train1)\n",
    "\n",
    "\n",
    "    sequences1 = tokenizer1.texts_to_sequences(x_train1)\n",
    "    \n",
    "    vocab_num1 = 0\n",
    "    for i in sequences1:\n",
    "        for j in i:\n",
    "            if vocab_num1 < j:\n",
    "                vocab_num1 = j\n",
    "                \n",
    "    x_train_seq1 = pad_sequences(sequences1, maxlen=max_len)\n",
    "    \n",
    "    sequences_val1 = tokenizer1.texts_to_sequences(x_test1)\n",
    "    x_val_seq1 = pad_sequences(sequences_val1, maxlen=max_len)\n",
    "    \n",
    "    \n",
    "    embeddings_index1 = {}\n",
    "    for w in model1.wv.vocab.keys():\n",
    "        embeddings_index1[w] = model1.wv[w]\n",
    "        \n",
    "    embedding_matrix1 = np.zeros((vocab_num1+1, 100))\n",
    "    for word, i in tokenizer1.word_index.items():\n",
    "        embedding_vector1 = embeddings_index1.get(word)\n",
    "        if embedding_vector1 is not None:\n",
    "            embedding_matrix1[i] = embedding_vector1\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    classifier1 = Sequential()\n",
    "    e1 = Embedding(vocab_num1+1, 100, weights=[embedding_matrix1], input_length=max_len, trainable=True)\n",
    "    classifier1.add(e1)\n",
    "    classifier1.add(Flatten())\n",
    "    classifier1.add(Dense(256, activation='relu'))\n",
    "    classifier1.add(Dense(nb_classes, activation='softmax'))\n",
    "    classifier1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    monitor1 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "            verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    classifier1.fit(x_train_seq1, y_train1, validation_data=(x_val_seq1, y_test1), \n",
    "            callbacks=[monitor1], epochs=30, batch_size=32, verbose=1)\n",
    "    \n",
    "    \n",
    "    pred = classifier1.predict(x_test1)\n",
    "    \n",
    "    oos_y.append(y_test1)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)  \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test1,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gensim.models\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keep a 10% holdout\n",
    "x_main, x_holdout, y_main, y_holdout = train_test_split(\n",
    "    X, Y, test_size=.02, random_state=SEED, stratify = Y)\n",
    "\n",
    "encoded_Y_main = encoder.transform(y_main)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_main = to_categorical(encoded_Y_main)\n",
    "\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42) \n",
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "# Must specify y StratifiedKFold for\n",
    "for train, test in kf.split(x_main, y_main):  \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train1 = x_main[train].astype(str)\n",
    "    y_train1 = dummy_y_main[train]\n",
    "    x_test1 = x_main[test]\n",
    "    y_test1 = dummy_y_main[test]\n",
    "    sentences1 = [row.split(' ') for row in x_train1]\n",
    "    model1 = gensim.models.Word2Vec(sentences1, min_count=1,size= 100,workers=3, window =3, sg = 1)\n",
    "\n",
    "    tokenizer1 = Tokenizer()\n",
    "    tokenizer1.fit_on_texts(x_train1)\n",
    "\n",
    "\n",
    "    sequences1 = tokenizer1.texts_to_sequences(x_train1)\n",
    "    \n",
    "    vocab_num1 = 0\n",
    "    for i in sequences1:\n",
    "        for j in i:\n",
    "            if vocab_num1 < j:\n",
    "                vocab_num1 = j\n",
    "                \n",
    "    x_train_seq1 = pad_sequences(sequences1, maxlen=max_len)\n",
    "    \n",
    "    sequences_val1 = tokenizer1.texts_to_sequences(x_test1)\n",
    "    x_val_seq1 = pad_sequences(sequences_val1, maxlen=max_len)\n",
    "    \n",
    "    \n",
    "    embeddings_index1 = {}\n",
    "    for w in model1.wv.vocab.keys():\n",
    "        embeddings_index1[w] = model1.wv[w]\n",
    "        \n",
    "    embedding_matrix1 = np.zeros((vocab_num1+1, 100))\n",
    "    for word, i in tokenizer1.word_index.items():\n",
    "        embedding_vector1 = embeddings_index1.get(word)\n",
    "        if embedding_vector1 is not None:\n",
    "            embedding_matrix1[i] = embedding_vector1\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    classifier1 = Sequential()\n",
    "    e1 = Embedding(vocab_num1+1, 100, weights=[embedding_matrix1], input_length=max_len, trainable=True)\n",
    "    classifier1.add(e1)\n",
    "    classifier1.add(Flatten())\n",
    "    classifier1.add(Dense(256, activation='relu'))\n",
    "    classifier1.add(Dense(nb_classes, activation='softmax'))\n",
    "    classifier1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    monitor1 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "            verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    classifier1.fit(x_train_seq1, y_train1, validation_data=(x_val_seq1, y_test1), \n",
    "            callbacks=[monitor1], epochs=30, batch_size=32, verbose=1)\n",
    "    \n",
    "    \n",
    "    pred = classifier1.predict(x_test1)\n",
    "    \n",
    "    oos_y.append(y_test1)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)  \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test1,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print()\n",
    "print(f\"Cross-validated score (RMSE): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction (from the last neural network)\n",
    "holdout_pred = model.predict(x_holdout)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "print(f\"Holdout score (RMSE): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
